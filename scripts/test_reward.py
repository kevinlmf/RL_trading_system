import numpy as np
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from learning.env.trading_env import TradingEnv
from exploration.latent_bonus import LatentFactorBonus

# === åˆå§‹åŒ–ç¯å¢ƒ ===
env = TradingEnv(data_source="simulated", window_size=30)
obs = env.reset()
print("ğŸ§© Initial Observation Shape:", obs.shape)

# === åˆå§‹åŒ–æ¢ç´¢å¥–åŠ±æ¨¡å— ===
bonus_fn = LatentFactorBonus(n_components=2, beta=0.05, bandwidth=0.3)

# === åˆå§‹åŒ–ç»Ÿè®¡é‡ ===
total_reward = 0.0
total_bonus = 0.0
step_count = 0

# === æ¨¡æ‹Ÿä¸€ä¸ª episode ===
done = False
while not done:
    action = np.random.uniform(-1, 1, size=env.asset_dim)  # éšæœºåŠ¨ä½œï¼ˆä¹Ÿå¯ä»¥æ¢æˆå…¶ä»–ç­–ç•¥ï¼‰
    
    obs, reward, done, info = env.step(action)
    
    bonus_fn.update_memory(obs)
    if step_count % 10 == 0:  # æ¯ 10 æ­¥æ›´æ–°ä¸€æ¬¡ KDE
        bonus_fn.fit_latent_space()

    bonus = bonus_fn.compute_bonus(obs)
    total_reward += reward
    total_bonus += bonus
    step_count += 1

    print(f"Step {step_count:03d} | Reward: {reward:.2f} | Bonus: {bonus:.2f} | PV: {info['portfolio_value']:.2f}")

# === æ€»ç»“ ===
print("\nâœ… Episode finished.")
print(f"ğŸ“ˆ Final Portfolio Value: {info['portfolio_value']:.2f}")
print(f"ğŸ¯ Total Reward: {total_reward:.2f}")
print(f"âœ¨ Total Exploration Bonus: {total_bonus:.2f}")

