import os
import sys
import gymnasium as gym
import pandas as pd
from datetime import datetime
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.evaluation import evaluate_policy

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "env")))
from env.trading_env import TradingEnv
from env.data_loader import load_csv_data

# âœ… åŠ è½½æ•°æ®
df = load_csv_data("data/SPY_1d.csv")

# âœ… æ„å»ºç¯å¢ƒ + Monitor
env = TradingEnv(data=df, window_size=10)
env = Monitor(env)

# âœ… æ—¶é—´æˆ³ + æ¨¡å‹ä¿å­˜è·¯å¾„
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
model_path = f"models/ppo_trading_{timestamp}"
tensorboard_log_dir = f"tensorboard/ppo_run_{timestamp}"

# âœ… åˆ›å»º PPO æ¨¡å‹
model = PPO(
    "MlpPolicy",
    env,
    learning_rate=1e-4,
    n_steps=2048,
    batch_size=64,
    gae_lambda=0.95,
    gamma=0.99,
    ent_coef=0.0,
    vf_coef=0.5,
    max_grad_norm=0.5,
    verbose=1,
    tensorboard_log=tensorboard_log_dir
)

# âœ… è®­ç»ƒå‰è¯„ä¼°
mean_reward_before, _ = evaluate_policy(model, env, n_eval_episodes=5)
print(f"ğŸ“Š Before training - Mean Reward: {mean_reward_before:.2f}")

# âœ… å¼€å§‹è®­ç»ƒ
model.learn(total_timesteps=10000)

# âœ… ä¿å­˜æ¨¡å‹
os.makedirs("models", exist_ok=True)
model.save(model_path)
print(f"âœ… PPO Model saved to {model_path}")

# âœ… è®­ç»ƒåè¯„ä¼°
mean_reward_after, _ = evaluate_policy(model, env, n_eval_episodes=5)
print(f"ğŸ“Š After training - Mean Reward: {mean_reward_after:.2f}")

