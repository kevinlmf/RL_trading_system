# RL_trading_system

This project implements a modular reinforcement learning (RL) trading system with multiple strategies including PPO, DQN, and Random policies. It features a custom trading environment and supports evaluation on both simulated (Copula-based) and real financial data.

---

## ğŸ“ Project Structure

```
RL_trading_system/
â”‚
â”œâ”€â”€ scripts/              # Training, testing, and data download scripts
â”‚
â”œâ”€â”€ cpp_implementation/   # (Optional) High-performance C++ modules (if used)
â”‚
â”œâ”€â”€ theory/               # LaTeX documents: financial math, RL theory, copula modeling
â”‚
â”œâ”€â”€ 3_data/
â”‚   â”œâ”€â”€ low_dimension/      # Simulated or real market data
â”‚   â””â”€â”€ processed/          # Cleaned CSVs
â”‚
â”œâ”€â”€ 4_learning/
â”‚   â”œâ”€â”€ env/                # Custom TradingEnv implementation
â”‚   â””â”€â”€ strategy/
â”‚       â”œâ”€â”€ rl/
â”‚       â”‚   â”œâ”€â”€ dqn/        # DQN agent and model
â”‚       â”‚   â”œâ”€â”€ ppo/        # PPO agent and model
â”‚       â”‚   â””â”€â”€ random/     # Random policy baseline
â”‚       â””â”€â”€ shared/         # Common reward functions, utilities, etc.
â”‚
â”œâ”€â”€ 5_evaluation/           # Scripts for evaluating and comparing strategies
â”‚
â”œâ”€â”€ Dockerfile              # Environment setup (optional)
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project introduction (this file)
```

---

## ğŸ§  Key Features

- **Custom Trading Environment:** With position tracking, account balance, and action history
- **Multiple Strategies:** PPO, DQN, and a Random policy baseline
- **Copula-Simulated Market Data:** For structured risk modeling
- **Evaluation Metrics:** Total reward, Sharpe ratio, maximum drawdown
- **Modular Design:** Clean separation of strategy, environment, and evaluation

---

## ğŸš€ Getting Started

### 1. Clone this repository
```bash
git clone https://github.com/kevinlmf/RL_trading_system.git
cd RL_trading_system
```

### 2. Set up the environment
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Run training (e.g., PPO)
```bash
python 0_scripts/train_ppo.py
```

### 4. Evaluate strategies
```bash
python 5_evaluation/evaluate_strategies.py
```

---

## ğŸ§© Notes

- PPO uses Generalized Advantage Estimation (GAE)
- DQN uses epsilon-greedy exploration with replay buffer
- Trading environment supports discrete Buy / Hold / Sell actions
- Evaluation averages over multiple episodes

---

## ğŸ“¬ Contact

Feel free to reach out via [GitHub Issues](https://github.com/kevinlmf/RL_trading_system/issues) or [LinkedIn](https://www.linkedin.com/in/yourprofile/).
